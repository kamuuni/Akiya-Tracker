import os
import requests
from bs4 import BeautifulSoup
import re
from supabase import create_client, Client


SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# --- 1. æ¥ç¶šè¨­å®š  ---
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

TARGET_URL = "https://www.city.niimi.okayama.jp/akurashi/customer/customer_search"

def scrape_niimi_list():
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(TARGET_URL, headers=headers)
    response.encoding = response.apparent_encoding
    soup = BeautifulSoup(response.text, 'html.parser')

    # ç‰©ä»¶ã‚’ã™ã¹ã¦å–å¾—
    property_cards = soup.select('.p-bukken')
    
    results = []

    for card in property_cards:
        try:
            # 1. ç™»éŒ²ç•ªå·ã®å–å¾—
            id_dt = card.find('dt', string=re.compile('ç™»éŒ²ç•ªå·'))
            property_id = id_dt.find_next_sibling('dd').get_text(strip=True) if id_dt else None
            if not property_id: continue

            # 2. è²©å£²ä¾¡æ ¼ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè³ƒæ–™ã—ã‹ãªã„ç‰©ä»¶ã¯ç„¡è¦–ã™ã‚‹ï¼‰
            price_dt = card.find('dt', string='è²©å£²ä¾¡æ ¼') # å®Œå…¨ä¸€è‡´ã§ã€Œè²©å£²ä¾¡æ ¼ã€ã‚’æ¢ã™
            if not price_dt:
                print(f"ã‚¹ã‚­ãƒƒãƒ—ï¼šç™»éŒ²ç•ªå·{property_id} ã¯è³ƒè²¸ç‰©ä»¶ã®ã‚ˆã†ã§ã™ã€‚")
                continue

            price_text = price_dt.find_next_sibling('dd').get_text(strip=True)
            
            # 3. ä¾¡æ ¼ã®æ•°å€¤åŒ–
            raw_number_match = re.search(r'([\d,.]+)', price_text)
            price_val = 0
            if raw_number_match:
                raw_number = float(raw_number_match.group(1).replace(',', ''))
                # å˜ä½ã«å¿œã˜ãŸè¨ˆç®—(ä¸‡ã¨åƒã®ã¿)
                if "ä¸‡" in price_text:
                    price_val = int(raw_number * 10000)
                elif "åƒ" in price_text:
                    price_val = int(raw_number * 1000)
                else:
                    price_val = int(raw_number)

            # 4. æ‰€åœ¨åœ°
            loc_dt = card.find('dt', string=re.compile('æ‰€åœ¨åœ°'))
            location = loc_dt.find_next_sibling('dd').get_text(strip=True) if loc_dt else "æ–°è¦‹å¸‚"

            # 5. è©³ç´°URL (ã€Œè©³ã—ãè¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã®ãƒªãƒ³ã‚¯)
            link_tag = card.find('a', string=re.compile('è©³ã—ãè¦‹ã‚‹'))
            detail_url = link_tag['href'] if link_tag else TARGET_URL

            # 6. ã‚¿ã‚¤ãƒˆãƒ«ã®ç”Ÿæˆ (ç™»éŒ²ç•ªå·ã¨æ‰€åœ¨åœ°ã‚’çµ„ã¿åˆã‚ã›ã‚‹)
            title = f"ç™»éŒ²ç•ªå·{property_id}ï¼ˆ{location}ï¼‰"

            results.append({
                "id": f"niimi_{property_id}",
                "title": title,
                "price": price_val,
                "status": "å…¬é–‹ä¸­",
                "url": detail_url
            })
        except Exception as e:
            print(f"1ä»¶è§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return results

def save_to_supabase(data_list):
    for data in data_list:
        if data['price'] <= 0:
            continue

        # 1. ã¾ãšã€ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã€Œå‰å›ã®ä¾¡æ ¼ã€ã‚’å¼•ã„ã¦ãã‚‹
        existing_data = supabase.table("properties") \
            .select("price") \
            .eq("id", data['id']) \
            .execute()

        # ç‰©ä»¶ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if existing_data.data:
            old_price = existing_data.data[0]['price']
            new_price = data['price']
            diff = old_price - new_price # å€¤ä¸‹ãŒã‚Šé¡

            # 2. ä¾¡æ ¼ã«å¤‰æ›´ãŒã‚ã£ãŸã‹ï¼Ÿ
            if old_price != new_price:
                # ä¾¡æ ¼ãŒå¤‰ã‚ã£ãŸã®ã§ã€æœ€æ–°æƒ…å ±ã‚’æ›´æ–°ï¼ˆupsertï¼‰
                supabase.table("properties").upsert(data).execute()

                # å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆprice_historyï¼‰ã«å¤ã„ä¾¡æ ¼ã‚’è¨˜éŒ²
                history_record = {
                    "property_id": data['id'],
                    "price": new_price,
                    "changed_at": "now()" # Supabaseå´ã§ç¾åœ¨æ™‚åˆ»ã‚’ã„ã‚Œã‚‹è¨­å®šãªã‚‰
                }
                supabase.table("price_history").insert(history_record).execute()

                # 3. ã€æ ¸å¿ƒã€‘10ä¸‡å††ä»¥ä¸Šã®å€¤ä¸‹ã’ã‹åˆ¤å®š
                if diff >= 100000:
                    print(f"ğŸ”¥ å¤§å¹…å€¤ä¸‹ã’æ¤œçŸ¥ï¼: {data['title']}")
                    print(f"   {old_price:,}å†† â†’ {new_price:,}å†† (â–²{diff:,}å††)")
                else:
                    print(f"âœ¨ ä¾¡æ ¼å¤‰æ›´: {data['title']} ({old_price:,}å†† â†’ {new_price:,}å††)")
            else:
                # ä¾¡æ ¼ãŒå¤‰ã‚ã£ã¦ã„ãªã„ãªã‚‰ã€ç”Ÿå­˜ç¢ºèªï¼ˆãƒã‚§ãƒƒã‚¯æ™‚åˆ»ï¼‰ã ã‘æ›´æ–°
                # ï¼ˆä»Šã®ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆã ã¨upsertã—ã¡ã‚ƒã†ã®ãŒä¸€ç•ªæ¥½ã§ã™ï¼‰
                supabase.table("properties").upsert(data).execute()
        
        else:
            # æ–°ç€ç‰©ä»¶ã®å ´åˆ
            supabase.table("properties").upsert(data).execute()
            print(f"ğŸ†• æ–°ç€ç‰©ä»¶ï¼: {data['title']} / {data['price']:,}å††")

if __name__ == "__main__":
    print(f"--- æ–°è¦‹å¸‚å…¬å¼ï¼šãƒ‡ãƒ¼ã‚¿åŒæœŸé–‹å§‹ ---")
    akiya_list = scrape_niimi_list()
    print(f"è§£ææˆåŠŸ: {len(akiya_list)} ä»¶ã®ç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    save_to_supabase(akiya_list)