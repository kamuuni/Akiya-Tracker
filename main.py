import os
import requests
from bs4 import BeautifulSoup
import re
from supabase import create_client, Client


# --- 1. æ¥ç¶šè¨­å®š  ---
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- LINEè¨­å®š ---
LINE_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN")
LINE_USER_ID = os.environ.get("LINE_USER_ID")

# --- ã“ã“ã‹ã‚‰æ–°è¦‹å¸‚ã®ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ---
TARGET_URL = "https://www.city.niimi.okayama.jp/akurashi/customer/customer_search"

def send_line_push(message):
    """LINE Messaging APIã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒƒã‚·ãƒ¥é€šçŸ¥ã‚’é€ä¿¡ã™ã‚‹"""
    if not LINE_TOKEN or not LINE_USER_ID:
        print("âš ï¸ LINE SecretsãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€é€šçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    url = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_TOKEN}"
    }
    payload = {
        "to": LINE_USER_ID,
        "messages": [{"type": "text", "text": message}]
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        if response.status_code != 200:
            print(f"âŒ LINEé€ä¿¡å¤±æ•—: {response.text}")
    except Exception as e:
        print(f"âŒ LINEé€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

def scrape_niimi_list():
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(TARGET_URL, headers=headers)
    response.encoding = response.apparent_encoding
    soup = BeautifulSoup(response.text, 'html.parser')

    # ç‰©ä»¶ã‚’ã™ã¹ã¦å–å¾—
    property_cards = soup.select('.p-bukken')
    
    results = []

    for card in property_cards:
        try:
            # 1. ç™»éŒ²ç•ªå·ã®å–å¾—
            id_dt = card.find('dt', string=re.compile('ç™»éŒ²ç•ªå·'))
            property_id = id_dt.find_next_sibling('dd').get_text(strip=True) if id_dt else None
            if not property_id: continue

            # 2. è²©å£²ä¾¡æ ¼ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè³ƒæ–™ã—ã‹ãªã„ç‰©ä»¶ã¯ç„¡è¦–ã™ã‚‹ï¼‰
            price_dt = card.find('dt', string='è²©å£²ä¾¡æ ¼') # å®Œå…¨ä¸€è‡´ã§ã€Œè²©å£²ä¾¡æ ¼ã€ã‚’æ¢ã™
            if not price_dt:
                print(f"ã‚¹ã‚­ãƒƒãƒ—ï¼šç™»éŒ²ç•ªå·{property_id} ã¯è³ƒè²¸ç‰©ä»¶ã®ã‚ˆã†ã§ã™ã€‚")
                continue

            price_text = price_dt.find_next_sibling('dd').get_text(strip=True)
            
            # 3. ä¾¡æ ¼ã®æ•°å€¤åŒ–
            raw_number_match = re.search(r'([\d,.]+)', price_text)
            price_val = 0
            if raw_number_match:
                raw_number = float(raw_number_match.group(1).replace(',', ''))
                # å˜ä½ã«å¿œã˜ãŸè¨ˆç®—(ä¸‡ã¨åƒã®ã¿)
                if "ä¸‡" in price_text:
                    price_val = int(raw_number * 10000)
                elif "åƒ" in price_text:
                    price_val = int(raw_number * 1000)
                else:
                    price_val = int(raw_number)

            # 4. æ‰€åœ¨åœ°
            loc_dt = card.find('dt', string=re.compile('æ‰€åœ¨åœ°'))
            location = loc_dt.find_next_sibling('dd').get_text(strip=True) if loc_dt else "æ–°è¦‹å¸‚"

            # 5. è©³ç´°URL (ã€Œè©³ã—ãè¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã®ãƒªãƒ³ã‚¯)
            link_tag = card.find('a', string=re.compile('è©³ã—ãè¦‹ã‚‹'))
            detail_url = link_tag['href'] if link_tag else TARGET_URL

            # 6. ã‚¿ã‚¤ãƒˆãƒ«ã®ç”Ÿæˆ (ç™»éŒ²ç•ªå·ã¨æ‰€åœ¨åœ°ã‚’çµ„ã¿åˆã‚ã›ã‚‹)
            title = f"ç™»éŒ²ç•ªå·{property_id}ï¼ˆ{location}ï¼‰"

            results.append({
                "id": f"niimi_{property_id}",
                "title": title,
                "price": price_val,
                "status": "å…¬é–‹ä¸­",
                "url": detail_url
            })
        except Exception as e:
            print(f"1ä»¶è§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return results

def save_to_supabase(data_list):
    for data in data_list:
        if data['price'] <= 0:
            continue

        # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª [cite: 302]
        existing_data = supabase.table("properties") \
            .select("price") \
            .eq("id", data['id']) \
            .execute()

        # A. ç‰©ä»¶ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹å ´åˆ [cite: 303]
        if existing_data.data:
            old_price = existing_data.data[0]['price']
            new_price = data['price']
            diff = old_price - new_price 

            if old_price != new_price:
                # æœ€æ–°æƒ…å ±ã«æ›´æ–°ã—ã€å±¥æ­´ã«ä¿å­˜ [cite: 304, 336]
                supabase.table("properties").upsert(data).execute()
                history_record = {
                    "property_id": data['id'],
                    "price": new_price,
                    "changed_at": "now()" 
                }
                supabase.table("price_history").insert(history_record).execute() [cite: 305]

                # é€šçŸ¥åˆ¤å®š [cite: 306, 337]
                if diff >= 100000:
                    msg = f"ğŸ”¥ ã€å¤§å¹…å€¤ä¸‹ã’ã€‘\n{data['title']}\n{old_price:,}å†† â†’ {new_price:,}å†† (â–²{diff:,}å††)\n{data['url']}"
                    send_line_push(msg)
                else:
                    msg = f"âœ¨ ã€ä¾¡æ ¼å¤‰æ›´ã€‘\n{data['title']}\n{old_price:,}å†† â†’ {new_price:,}å††"
                    send_line_push(msg) [cite: 338]
            else:
                # ä¾¡æ ¼å¤‰æ›´ãªã—ã€‚ç”Ÿå­˜ç¢ºèªã¨ã—ã¦æ›´æ–° 
                supabase.table("properties").upsert(data).execute()
        
        # B. æ–°ç€ç‰©ä»¶ã®å ´åˆ [cite: 307, 339]
        else:
            supabase.table("properties").upsert(data).execute() [cite: 339]
            # æ–°ç€é€šçŸ¥ã‚’é€ä¿¡
            msg = f"ğŸ†• ã€æ–°ç€ç‰©ä»¶ï¼ã€‘\n{data['title']}\nä¾¡æ ¼: {data['price']:,}å††\n{data['url']}"
            print(msg) [cite: 339]
            send_line_push(msg) # ã“ã“ã§LINEé€šçŸ¥

if __name__ == "__main__":
    print(f"--- æ–°è¦‹å¸‚å…¬å¼ï¼šãƒ‡ãƒ¼ã‚¿åŒæœŸé–‹å§‹ ---")
    akiya_list = scrape_niimi_list()
    print(f"è§£ææˆåŠŸ: {len(akiya_list)} ä»¶ã®ç‰©ä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    save_to_supabase(akiya_list)